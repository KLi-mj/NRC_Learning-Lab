import arxiv
import pandas as pd
import requests
import time
from datetime import datetime
import re
import os

class PaperCollector:
    def __init__(self):
        self.papers = []
        
    def search_arxiv_papers(self, query, max_results=30):
        """arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰ ë° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘"""
        print(f"ğŸ” arXivì—ì„œ '{query}' í‚¤ì›Œë“œë¡œ {max_results}ê°œ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘...")
        
        # arXiv ê²€ìƒ‰ ì„¤ì •
        search = arxiv.Search(
            query=query,
            max_results=max_results,
            sort_by=arxiv.SortCriterion.Relevance,
            sort_order=arxiv.SortOrder.Descending
        )
        
        papers_data = []
        
        try:
            for i, paper in enumerate(search.results(), 1):
                print(f"ğŸ“„ {i}/{max_results}: {paper.title[:50]}...")
                
                # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                paper_info = {
                    'id': i,
                    'arxiv_id': paper.get_short_id(),
                    'title': paper.title.strip(),
                    'authors': ', '.join([author.name for author in paper.authors]),
                    'published_date': paper.published.strftime('%Y-%m-%d'),
                    'categories': ', '.join(paper.categories),
                    'primary_category': paper.primary_category,
                    'abstract': paper.summary.strip().replace('\n', ' '),
                    'pdf_url': paper.pdf_url,
                    'arxiv_url': paper.entry_id,
                    'comment': getattr(paper, 'comment', ''),
                    'journal_ref': getattr(paper, 'journal_ref', ''),
                    'doi': getattr(paper, 'doi', ''),
                    'word_count': len(paper.summary.split()),
                    'collected_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                papers_data.append(paper_info)
                
                # API ì œí•œ ê³ ë ¤í•œ ë”œë ˆì´
                time.sleep(0.5)
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
        self.papers = papers_data
        print(f"âœ… ì´ {len(papers_data)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ!")
        return papers_data
    
    def classify_papers_by_category(self):
        """ë…¼ë¬¸ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        if not self.papers:
            print("âŒ ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        df = pd.DataFrame(self.papers)
        
        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
        category_mapping = {
            'cs.AI': 'Artificial Intelligence',
            'cs.LG': 'Machine Learning', 
            'cs.CV': 'Computer Vision',
            'cs.CL': 'Natural Language Processing',
            'cs.RO': 'Robotics',
            'cs.IR': 'Information Retrieval',
            'cs.NE': 'Neural Networks',
            'stat.ML': 'Statistical ML',
            'cs.DC': 'Distributed Computing',
            'cs.CR': 'Cryptography & Security'
        }
        
        def get_main_category(categories):
            for cat in categories.split(', '):
                if cat in category_mapping:
                    return category_mapping[cat]
            return 'Other'
        
        df['main_category'] = df['categories'].apply(get_main_category)
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = df['main_category'].value_counts()
        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ë…¼ë¬¸ ë¶„í¬:")
        for category, count in category_stats.items():
            print(f"  {category}: {count}ê°œ")
            
        return df
    
    def save_to_excel(self, df, filename='ai_papers_pilot.xlsx'):
        """ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ì—´ ìˆœì„œ ì •ë¦¬
            columns_order = [
                'id', 'title', 'authors', 'published_date', 'main_category', 
                'primary_category', 'categories', 'abstract', 'word_count',
                'arxiv_id', 'pdf_url', 'arxiv_url', 'journal_ref', 'doi', 
                'comment', 'collected_at'
            ]
            
            df_ordered = df[columns_order]
            
            # ì—‘ì…€ ì €ì¥
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ì „ì²´ ë°ì´í„°
                df_ordered.to_excel(writer, sheet_name='ì „ì²´ë…¼ë¬¸', index=False)
                
                # ì¹´í…Œê³ ë¦¬ë³„ ì‹œíŠ¸
                for category in df['main_category'].unique():
                    category_df = df_ordered[df_ordered['main_category'] == category]
                    sheet_name = category.replace('/', '_')[:30]  # ì‹œíŠ¸ëª… ê¸¸ì´ ì œí•œ
                    category_df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # í†µê³„ ì‹œíŠ¸
                stats_df = df['main_category'].value_counts().reset_index()
                stats_df.columns = ['ì¹´í…Œê³ ë¦¬', 'ë…¼ë¬¸ìˆ˜']
                stats_df.to_excel(writer, sheet_name='í†µê³„', index=False)
            
            print(f"ğŸ’¾ ì—‘ì…€ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            print(f"âŒ ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def generate_summary_report(self, df):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“ˆ AI ë…¼ë¬¸ ìˆ˜ì§‘ íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸")
        print("="*60)
        
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: AI technology")
        print(f"ğŸ“š ì´ ìˆ˜ì§‘ ë…¼ë¬¸: {len(df)}ê°œ")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {df['published_date'].min()} ~ {df['published_date'].max()}")
        print(f"ğŸ“Š í‰ê·  ì´ˆë¡ ê¸¸ì´: {df['word_count'].mean():.0f} ë‹¨ì–´")
        
        print(f"\nğŸ·ï¸ ì£¼ìš” ì¹´í…Œê³ ë¦¬:")
        for category, count in df['main_category'].value_counts().head(5).items():
            percentage = (count / len(df)) * 100
            print(f"  â€¢ {category}: {count}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ“ ìµœì‹  ë…¼ë¬¸ 5ê°œ:")
        latest_papers = df.nlargest(5, 'published_date')
        for _, paper in latest_papers.iterrows():
            print(f"  â€¢ {paper['title'][:60]}... ({paper['published_date']})")
        
        print("\n" + "="*60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AI ë…¼ë¬¸ ìë™ë¶„ë¥˜ íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    
    # 1. ë…¼ë¬¸ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = PaperCollector()
    
    # 2. AIì™€ ê¸°ìˆ  ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
    # arXiv ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
    query = "artificial intelligence OR machine learning OR deep learning OR AI technology"
    papers = collector.search_arxiv_papers(query, max_results=30)
    
    if not papers:
        print("âŒ ë…¼ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    # 3. ë…¼ë¬¸ ë¶„ë¥˜
    df = collector.classify_papers_by_category()
    
    if df is None:
        return
    
    # 4. ì—‘ì…€ íŒŒì¼ ì €ì¥
    collector.save_to_excel(df)
    
    # 5. ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
    collector.generate_summary_report(df)
    
    print("\nâœ… íŒŒì¼ëŸ¿ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì´ì œ íŒ€ ë¹Œë”©ì— í™œìš©í•˜ì„¸ìš”! ğŸ¯")

if __name__ == "__main__":
    main()