import pandas as pd
import numpy as np
from openai import OpenAI
import time
from sklearn.cluster import KMeans
from config import Config

class PaperAnalyzer:
    """ë…¼ë¬¸ ë¶„ì„ê¸°: GPT ìš”ì•½ + í´ëŸ¬ìŠ¤í„°ë§ (ì‹œê°í™” ì œê±°)"""
    
    def __init__(self):
        self.client = OpenAI(api_key=Config.OPENAI_API_KEY)
        self.papers_df = None
        self.embeddings = None
        self.clusters = None
        
    def load_papers(self, excel_file):
        """ì—‘ì…€ íŒŒì¼ì—ì„œ ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ"""
        try:
            self.papers_df = pd.read_excel(excel_file, sheet_name='ì „ì²´ë…¼ë¬¸')
            print(f"ğŸ“š {len(self.papers_df)}ê°œ ë…¼ë¬¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
            return True
        except Exception as e:
            print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def summarize_abstracts_with_gpt(self):
        """GPTë¥¼ ì‚¬ìš©í•œ ì´ˆë¡ ìš”ì•½"""
        if self.papers_df is None:
            print("âŒ ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € load_papers()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        print("ğŸ¤– GPTë¡œ ì´ˆë¡ ìš”ì•½ ì¤‘...")
        summaries = []
        key_insights = []
        
        for i, row in self.papers_df.iterrows():
            print(f"ğŸ“ {i+1}/{len(self.papers_df)}: {row['title'][:40]}...")
            
            try:
                # ìš”ì•½ í”„ë¡¬í”„íŠ¸
                summary_prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ ì´ˆë¡ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš” (2-3ë¬¸ì¥):

ì œëª©: {row['title']}
ì´ˆë¡: {row['abstract']}

ìš”ì•½:"""

                # í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
                insight_prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì—ì„œ í•µì‹¬ ê¸°ìˆ ì´ë‚˜ ë°©ë²•ë¡ ì„ 1-2ê°œ í‚¤ì›Œë“œë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œëª©: {row['title']}
ì´ˆë¡: {row['abstract']}

í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„):"""

                # GPT API í˜¸ì¶œ
                summary_response = self.client.chat.completions.create(
                    model=Config.GPT_CONFIG['model'],
                    messages=[{"role": "user", "content": summary_prompt}],
                    max_tokens=Config.GPT_CONFIG['max_tokens'],
                    temperature=Config.GPT_CONFIG['temperature']
                )
                
                time.sleep(1)  # API ì œí•œ ê³ ë ¤
                
                insight_response = self.client.chat.completions.create(
                    model=Config.GPT_CONFIG['model'],
                    messages=[{"role": "user", "content": insight_prompt}],
                    max_tokens=50,
                    temperature=Config.GPT_CONFIG['temperature']
                )
                
                summary = summary_response.choices[0].message.content.strip()
                insight = insight_response.choices[0].message.content.strip()
                
                summaries.append(summary)
                key_insights.append(insight)
                
                time.sleep(1)  # API ì œí•œ ê³ ë ¤
                
            except Exception as e:
                print(f"âš ï¸ {i+1}ë²ˆ ë…¼ë¬¸ ìš”ì•½ ì‹¤íŒ¨: {e}")
                summaries.append("ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
                key_insights.append("í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
        
        # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
        self.papers_df['gpt_summary'] = summaries
        self.papers_df['key_insights'] = key_insights
        
        print("âœ… GPT ìš”ì•½ ì™„ë£Œ!")
        return self.papers_df
    
    def create_embeddings(self):
        """OpenAI ì„ë² ë”© ìƒì„±"""
        if self.papers_df is None:
            print("âŒ ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        print("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # ì œëª© + ì´ˆë¡ì„ ê²°í•©í•œ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìƒì„±
        texts = []
        for _, row in self.papers_df.iterrows():
            combined_text = f"{row['title']} {row['abstract']}"
            texts.append(combined_text)
        
        try:
            embeddings = []
            batch_size = 10  # ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì í™”
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                print(f"ğŸ“Š ì„ë² ë”© ìƒì„±: {i+1}-{min(i+batch_size, len(texts))}/{len(texts)}")
                
                response = self.client.embeddings.create(
                    model=Config.CLUSTERING_CONFIG['embedding_model'],
                    input=batch
                )
                
                for embedding_obj in response.data:
                    embeddings.append(embedding_obj.embedding)
                
                time.sleep(1)  # API ì œí•œ ê³ ë ¤
            
            self.embeddings = np.array(embeddings)
            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! ì°¨ì›: {self.embeddings.shape}")
            return self.embeddings
            
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def perform_clustering(self, n_clusters=None):
        """K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        if self.embeddings is None:
            print("âŒ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € create_embeddings()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        if n_clusters is None:
            n_clusters = Config.CLUSTERING_CONFIG['n_clusters']
        
        print(f"ğŸ¯ {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜ ì¤‘...")
        
        try:
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(self.embeddings)
            
            # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
            self.papers_df['cluster'] = cluster_labels
            self.clusters = cluster_labels
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
            cluster_stats = pd.Series(cluster_labels).value_counts().sort_index()
            print("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ë…¼ë¬¸ ìˆ˜:")
            for cluster_id, count in cluster_stats.items():
                print(f"  í´ëŸ¬ìŠ¤í„° {cluster_id}: {count}ê°œ")
            
            print("âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            return cluster_labels
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_clusters(self):
        """í´ëŸ¬ìŠ¤í„°ë³„ ì£¼ìš” íŠ¹ì§• ë¶„ì„"""
        if self.papers_df is None or 'cluster' not in self.papers_df.columns:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ” í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì§• ë¶„ì„ ì¤‘...")
        
        cluster_analysis = []
        
        for cluster_id in sorted(self.papers_df['cluster'].unique()):
            cluster_papers = self.papers_df[self.papers_df['cluster'] == cluster_id]
            
            # í´ëŸ¬ìŠ¤í„°ì˜ ì£¼ìš” íŠ¹ì§• ì¶”ì¶œ
            analysis = {
                'cluster_id': cluster_id,
                'paper_count': len(cluster_papers),
                'main_categories': cluster_papers['main_category'].value_counts().head(3).to_dict(),
                'avg_year': cluster_papers['published_date'].apply(lambda x: int(x[:4])).mean(),
                'sample_titles': cluster_papers['title'].head(3).tolist(),
                'common_keywords': self._extract_common_keywords(cluster_papers['key_insights'].tolist())
            }
            
            cluster_analysis.append(analysis)
            
            print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„° {cluster_id} ({len(cluster_papers)}ê°œ ë…¼ë¬¸):")
            print(f"  ì£¼ìš” ì¹´í…Œê³ ë¦¬: {list(analysis['main_categories'].keys())[:2]}")
            print(f"  í‰ê·  ë°œí–‰ë…„ë„: {analysis['avg_year']:.1f}")
            print(f"  ê³µí†µ í‚¤ì›Œë“œ: {analysis['common_keywords'][:3]}")
        
        return cluster_analysis
    
    def _extract_common_keywords(self, keyword_lists):
        """ê³µí†µ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_keywords = []
        for keywords_str in keyword_lists:
            if isinstance(keywords_str, str):
                keywords = [k.strip().lower() for k in keywords_str.split(',')]
                all_keywords.extend(keywords)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        keyword_counts = {}
        for keyword in all_keywords:
            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ë°˜í™˜
        sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
        return [kw[0] for kw in sorted_keywords[:5]]
    
    def visualize_clusters(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ (matplotlib ì‚¬ìš© ì•ˆí•¨)"""
        if self.embeddings is None or self.clusters is None:
            print("âŒ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½:")
        print("-" * 40)
        
        unique_clusters = np.unique(self.clusters)
        for cluster_id in unique_clusters:
            cluster_papers = self.papers_df[self.papers_df['cluster'] == cluster_id]
            print(f"í´ëŸ¬ìŠ¤í„° {cluster_id}: {len(cluster_papers)}ê°œ ë…¼ë¬¸")
            print(f"  ëŒ€í‘œ ë…¼ë¬¸: {cluster_papers.iloc[0]['title'][:50]}...")
        
        print("ğŸ“Š ì‹œê°í™” ì°¨íŠ¸ëŠ” matplotlib ì˜¤ë¥˜ë¡œ ìƒëµë¨")
    
    def save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ì €ì¥"""
        if self.papers_df is None:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filename = Config.OUTPUT_CONFIG['excel_filename']
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ì „ì²´ ë¶„ì„ ê²°ê³¼
                self.papers_df.to_excel(writer, sheet_name='ë¶„ì„ê²°ê³¼', index=False)
                
                # í´ëŸ¬ìŠ¤í„°ë³„ ì‹œíŠ¸
                if 'cluster' in self.papers_df.columns:
                    for cluster_id in sorted(self.papers_df['cluster'].unique()):
                        cluster_data = self.papers_df[self.papers_df['cluster'] == cluster_id]
                        sheet_name = f'í´ëŸ¬ìŠ¤í„°_{cluster_id}'
                        cluster_data.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # ìš”ì•½ í†µê³„
                summary_stats = {
                    'í•­ëª©': ['ì´ ë…¼ë¬¸ìˆ˜', 'í‰ê·  ì´ˆë¡ ê¸¸ì´', 'í´ëŸ¬ìŠ¤í„° ìˆ˜', 'ë¶„ì„ ì™„ë£Œ ì‹œê°„'],
                    'ê°’': [
                        len(self.papers_df),
                        self.papers_df['word_count'].mean(),
                        len(self.papers_df['cluster'].unique()) if 'cluster' in self.papers_df.columns else 0,
                        pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
                    ]
                }
                pd.DataFrame(summary_stats).to_excel(writer, sheet_name='ìš”ì•½í†µê³„', index=False)
            
            print(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")